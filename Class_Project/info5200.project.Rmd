---
title: "Project: LMS Data Analytics"
author: '[[Cole Walsh, 4399966]]'
subtitle: 'INFO 5200 Learning Analytics'
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
# This loads 3 datasets: cl=clickstream, a=assessment grades; m=module states.
load("edx_lms_feb25.rda")
```

# Introduction

**Disclaimer:** Think about this project like this: in the workplace you can certainly talk to your peers about problems and work through them, but they won't write your code. This is an individual student project. Each student needs to write and submit their own work. You may talk to other students but not copy their code. Be generous in acknowledging where a conversation with another student helped you by adding: `H/T Student Name`.

**Project Goal:** The goal of this project is to learn how to work with Learning Management System (LMS) data and apply some of the prediction skills you have learned so far. This is an export of the class's edX data from Jan 23 to Feb 23. I have done some data cleaning for you and anonymized the datasets. However, there is plenty of real-world messiness for you to tackle. As always, you should start by getting to know the datasets. In this case, you should be able to really understand what is going on because it is your data and your know the platform. Moreover, you can navigate the relevant pages on edX to see what page/action the data refers to.

# Step 1: Understand the data

There are three datasets which can be connected using the hash_id column (a hashed version of the user id):

1. Clickstream data (1 row per student per action): [click for documentation](https://edx.readthedocs.io/projects/devdata/en/stable/internal_data_formats/tracking_logs.html#tracking-logs)
2. Module States (1 row per student per accessed content): original name [courseware-studentmodule (click for doumentation)](https://edx.readthedocs.io/projects/devdata/en/stable/internal_data_formats/sql_schema.html#courseware-studentmodule)
4. Assessment grades (1 row per assessment per student)

Note that I have already converted date-time objects into a numeric `timestamp` for you.

In the space below, explore each dataset e.g. using `head()`, `str()`, `summary()`, `table(data$column)`. You can also plot the distribution of variables with histograms or boxplots. Check out the data documentation linked above to understand the meaning of each column.

```{r}
############################################### 
###### BEGIN INPUT: Explore each dataset ###### 
###############################################

# Exploring Clickstreams
# add code here
head(cl)

# Exploring Module States
# add code here
head(m)

# Exploring Assessment grades
# add code here
head(a)

###############################################
###############################################
```

You may notice that it would be helpful to combine the information about grades and time of first attempt with the module state data. Below I make this join for you. See that only 'sequential' modules have grade data associated with them. The boxplot shows when the different sequentials (containing problems) were attempted. This gives you an idea of the order of problems in the course.

```{r}
# I left joined on a instead of m because if a student did not access any content they 
#would not have an associated observation for that module in m, but every student 
#received a grade on an assignment

ma =  a %>% select(hash_id:possible_graded, first_attempted_timestamp) %>% 
  left_join(m, by = c("hash_id"="hash_id", "usage_key" = "module_id")
)

colnames(ma)[colnames(ma)=="usage_key"] <- "module_id"

table(ma$module_type, ma$first_attempted_timestamp>0)
boxplot(ma$first_attempted_timestamp ~ ma$module_id)
```

# Step 2: Define a prediction task

Recall the guidelines for defining a good prediction problem covered in the Handbook chapter on prediction. You are looking for something actionable (an opportunity to intervene) and a situation that repeats (so the prediction can be useful in the future). The tradeoff with the dataset you have here is that on the one hand it is very relevant to you but on the other hand it is relatively small. Still, the data is fine-grained and sufficiently messy to give you a taste of LMS data analysis.

The prediction problem for this project is to build a one-day early warning system for missing a graded submission. Specifically, **your goal is to predict if one day before the deadline, whether a student will forget to submit an assignment**, so that the system can send a reminder. As you may have noticed during the data exploration phase above (if not, you should go back and examine this), there are several graded submissions and some students missed one or more of them. We define **missing a submission** as having an NA for `first_attempted_timestamp` but of course only for those that are past due.

### Instructions

1. Treat each graded assignment as a prediction task (thus there are x*n prediction opportunities where x = num. graded assignments and n = 31 students).
2. Create a dataset that has 1 row per student per graded assessment with the binary outcome (did they MISS it? yes/no) and several predictors (see next tip)
3. Predictors (i.e. features) need to be engineered with data from **24hrs before the assignment is due**, which of course varies across assignments; that means you have much more information to predict later assignments than earlier ones
4. Once your dataset is ready, split it into a training and a test set
5. Train a prediction model on the training data; you can try out any of the ones we have covered in the prediction homework and Random Forest
6. Keep tuning your model choice, model parameters (if any), and feature engineering
6. Finally, test your prediction accuracy on the test set

# Step 3: Getting you started

## Create the outcome variable

**Identify the graded assessments and whether a student did NOT submit**. Recall we want to have a *warning* system, so the outcome needs be the negative action.

Get the outcome for each graded assignment. Figure out the deadline for each and compute the timestamp for 24hrs prior to the deadline. You probably want to use the `ma` dataset I created for you above.

`r boxplot(ma$first_attempted_timestamp ~ ma$module_id)`

The following table helps you see the various graded assignments to consider. We keep only those where possible_graded > 0. **I define the deadline as the 90th percentile of submissions (you may use this simplification).**

```{r}
dat_deadline <- ma %>% 
    filter(possible_graded > 0) %>%
    group_by(module_id) %>% 
    summarise(
        deadline = quantile(first_attempted_timestamp, probs = .9, na.rm=T),
        p_unsubmitted = mean(is.na(first_attempted_timestamp))
    ) %>% 
    arrange(deadline) %>%
  filter(p_unsubmitted < 1)

dat_deadline
```

Now you know which assessments (module_ids) to target. **Be sure to kick out the one with p_unsubmitted = 1**; it gives you no information.

Now build a dataset with an indicator for each person and each of these module_ids with 1=unsubmitted, 0=submitted. Keep track of the deadline: you only want to use features in based on data up to 1 day before it (i.e. `24 * 60 * 60` seconds).

```{r}
############################################### 
####### BEGIN INPUT: Define outcome ###########
###############################################

# add code
ma_deadline <- inner_join(ma, dat_deadline, by = 'module_id') %>%
  mutate(unsubmitted = 1*is.na(first_attempted_timestamp))

# So there's this weird thing where two students have completed a certain module, but no 
# one else has and possible_graded = 1 for those 2 students and 0 for everybody else...
# I take it this is a module that was still open when this data was pulled and these two
# students were overachievers who completed it early, but its not necessarily unsubmitted 
# for the rest of the students...for the purposes of this project I'm going to drop this 
# module since this behaviour is different than the other 14 modules that have already 
# closed.

Open_Assessments <- ma_deadline %>%
  group_by(module_id) %>%
  summarize(N_Students_Open = sum(possible_graded == 0))

Open_Assessments

ma_deadline_NoOpen <- inner_join(ma_deadline, Open_Assessments, by = 'module_id') %>%
  filter(N_Students_Open == 0) %>%
  select(-c('N_Students_Open'))

# Thus, my dataframe has 31*14 = 434 rows

############################################### 
############################################### 
```

## Feature Engineering

**For each graded assessment, identify what data is appropriate for feature engineering**

Before you start feature engineering, you need to constrain the data for **each** assessment.

Remember the dataset we are aiming for has 1 row per person and assessment with several feature variables and one outcome variable. You created the outcome above. Now you need to create the appropriate features to join. I'm giving you an example for using `deadline = 1550655231` and creating 2 basic features from the clickstream. You should try to create a lot more features, incuding complex ones, that can use the clistream or other datasets (but remember the timing constraint).

```{r}
secs_day = 60 * 60 * 24
example_deadline = 1550655231

example_features = cl %>% 
    filter(timestamp < example_deadline - secs_day) %>%
    group_by(hash_id) %>%
    summarise(
        num_events = n(),
        num_seq_goto = sum(event_type=="seq_goto")
    )

head(example_features)
```

```{r}
############################################### 
###### BEGIN INPUT: Engineer features #########
###############################################

# add code here
# Creating features based on clickstream data, including total number of clicks before 
#deadline and for various time intervals before deadline, how many times a student has 
#jumped between sequences, and how many times they have saved and checked problems 
#including recently.

Click_Features_df <- left_join(ma_deadline_NoOpen, cl, by = 'hash_id') %>%
  filter(timestamp < deadline - secs_day) %>%
  group_by(hash_id, module_id) %>%
  summarize(TotalNum_Events = n(),
            Num_Events_last7days = sum(timestamp >= deadline - 8 * secs_day),
            Num_Events_last3days = sum(timestamp >= deadline - 4 * secs_day),
            Num_Events_last1day = sum(timestamp >= deadline - 2 * secs_day),
            Num_Seq_Goto = sum(event_type == 'seq_goto'),
            Num_Seq_Next = sum(event_type == 'seq_next'),
            Num_Seq_Prev = sum(event_type == 'seq_prev'),
            Num_Save = sum(event_type == "problem_save"),
            Num_Save_last1day = sum((timestamp >= deadline - 2 * secs_day) & 
                                      (event_type == "problem_save")),
            Num_Check = sum(event_type == "problem_check"),
            Num_Check_last1day = sum((timestamp >= deadline - 2 * secs_day) & 
                                       (event_type == "problem_check")))

# Creating features based on students submissions to modules due more than 24 hours before 
# the current deadline, including how many and what fraction of previous submissions (due 
# at least 24h earlier) a student missed and how many and what fraction of previous 
# submissions were submitted within the last 24h before the deadline.

library(data.table)
ma_deadline_copy = copy(ma_deadline_NoOpen)
ma_pastDeadlines <- left_join(ma_deadline_NoOpen, ma_deadline_copy, by = 'hash_id') %>%
  filter(deadline.y < deadline.x - secs_day) %>%
  group_by(hash_id, module_id.x) %>%
  summarize(N_prev_unsubmitted = sum(unsubmitted.y),
            frac_prev_unsubmitted = mean(unsubmitted.y),
            N_prev_submit_last24 = sum(first_attempted_timestamp.y > deadline.y - secs_day),
            frac_prev_submit_last24 = mean(first_attempted_timestamp.y > deadline.y - secs_day))

# I also keep the highest possible grade of the assigment as a predictor variable

Deadlines_Features_df <- left_join(ma_deadline_NoOpen[, c('hash_id', 'module_id',
                                                          'possible_graded', 'unsubmitted')],
                                   ma_pastDeadlines, by = c('hash_id', 'module_id' =
                                                              'module_id.x'))

df <- left_join(Deadlines_Features_df, Click_Features_df, by = c('hash_id', 'module_id')) %>%
  select(-c('hash_id', 'module_id'))
df[is.na(df)] <- 0
head(df)


###############################################
###############################################
```

# Step 4: Split your dataset

It is up to you how you choose to split the data but make sure you have enough to train on. You can look back at the prediction homework for how to do this.

```{r}
############################################### 
######## BEGIN INPUT: Split dataset ###########
###############################################

# add code here
ids_train = sample(nrow(df), size = nrow(df) * 0.8)
    
train = df[ids_train, ]
test = df[-ids_train, ]

###############################################
###############################################
```

# Step 5: Train your model(s)

1. Train one or more prediction models
2. Report the accuracy on the training data

```{r}
############################################### 
####### BEGIN INPUT: Train and report #########
###############################################

# add code here
library(caret) #imported for confusionMatrix function so I don't need to 
# manually compute accuracy, parameter tuning is still done manually for randomForest.
library(randomForest)
library(e1071)

# Check different values of mtry for highest in-sample accuracy in randomForest
print('Checking Accuracy of random forest with training data and various values for mtry...')
for (mtry in 1:(ncol(train) - 1)){
  rf_model <- randomForest(as.factor(unsubmitted) ~ ., data = train, mtry = mtry)
  rf_train_pred <- predict(rf_model, train)
  print(mtry)
  print(confusionMatrix(rf_train_pred, as.factor(train$unsubmitted))$overall[1])
}

# mtry = 4 gave the highest in-sample accuracy so I use mtry = 4 in my final rf model
rf_model <- randomForest(as.factor(unsubmitted) ~ ., data = train, mtry = 4)

log_model <- glm(unsubmitted ~ ., data = train, family = 'binomial')
log_train_pred <- predict(log_model, train, type = 'response')
print('Accuracy of logistic regression with training data...')
confusionMatrix(as.factor(1*(log_train_pred > 0.5)), as.factor(train$unsubmitted))$overall[1]

nb_model <- naiveBayes(as.factor(unsubmitted) ~ ., train)
nb_train_pred <- predict(nb_model, train)
print('Accuracy of Naive Bayes with training data...')
confusionMatrix(nb_train_pred, as.factor(train$unsubmitted))$overall[1]

###############################################
###############################################
```

# Step 6: Test your model(s)

1. Predict held-out test data with your model
2. Compute the accuracy of your model

```{r}
############################################### 
####### BEGIN INPUT: Test and report ##########
###############################################

# add code here
rf_pred <- predict(rf_model, test)
confusionMatrix(rf_pred, as.factor(test$unsubmitted))
varImpPlot(rf_model)

log_pred <- predict(log_model, newdata = test, type = 'response')
confusionMatrix(as.factor(1*(log_pred > 0.5)), as.factor(test$unsubmitted))
varImp(log_model)

nb_pred <- predict(nb_model, test)
confusionMatrix(nb_pred, as.factor(test$unsubmitted))

###############################################
###############################################
```

# Step 7: Report

Write down a brief report. Imagine your direct supervisor asked you to investigate the possibility of an early warning system. She would like to know what model you use, what features are important, and most importantly how well it would work. Given what you've learned, would you recommend implementing the system?

**Write your reponse answering the above questions below:**

%######## BEGIN INPUT: Summarize findings ############

Add your summary here.

The Naive Bayes model is terrible, don't use this. The logistic regression and random forest models both provide reasonable predictions, though it is worth pointing out that just predicting that everyone submitted every assignment in the test set would have given an accuracy of 88.5% so accuracy itself is not necessarily the best measure to use here for an unbalanced dataset. For this reason, I would argue that the random forest model is the one that should be implemented given that out of 10 unsubmitted assignments, it predicted 4 with a tradeoff of also having 4 false positives. I think this makes this system implementable. The random forest model indicates that the number of total clicks before the deadline, indicating active users, and the number of clicks closer to the deadline, indicating recently active users, are the most important...as is the point value of the assignment (see variable importance plot above). The logistic regression model found that three variables have an associated t-statistic greater than 2, indicating that these coefficients are significant at the alpha = 0.05 significance level. These variables were the number of assignments that were previously not submitted (again, only using assignments whose deadline was at least 24h before the assignment in question) the number of times problems were checked, and the number of times problems were checked in the time period 24-48h before the deadline.

%###############################################

# Submit Project

This is the end of the project. Please **Knit a PDF report** that shows both the R code and R output and upload it on the EdX platform. Alternatively, you can Knit it as a "doc", open it in Word, and save that as a PDF.

**Important:** Be sure that all your code is visible. If the line is too long, it gets cut off. If that happens, organize your code on several lines.
