---
title: "Predictive Models in Education"
author: '[[Cole Walsh, 4399966]]'
subtitle: 'INFO 5200 Learning Analytics: Week 4 Homework'
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## The Dataset

For this homework, you will be analyzing the Assisstments dataset from last homework. You should be familiar with the properties of the data at this point. If I gave you new dataset, you would most likely be going through some of the same steps as in the previous homework to get familiar with the dataset. Below I'm copying some of the general info about the dataset from before just in case:

The dataset provides question-level data of students practicing math problems in academic year 2004-2005 using the [Assisstments platform](https://www.assistments.org/). On this platform, students can attempt a problem many times to get it right and they can ask for more and more hints on a problem until the final hint tells them what the answer is. Based on the first few lines of data, and what we know about the dataset, we can infer the following:

- *studentID* is an identifier for students
- *itemid* is an identifier for math questions
- *correctonfirstattempt* is an indicator of whether a student answered correctly on the first attempt
- *attempts* is the number of answer attempts required
- *hints* the number of hints a student requested
- *seconds* time spent on the question in seconds
- the remaining columns provide start and end times and dates for each question

The dataset is in **long format** (1 row = 1 event) instead of wide format (1 row = 1 individual). However, as you can see from the *attempts* variable, you do not have data on each attempt, but a question-level rollup. The data is at the student-question level, which means that there is one row for each question a student attempted that summarizes interaction with the question (performance indicators and time spent).

Start by loading the dataset:
```{r}
library(tidyverse, quietly = T)
# load info5200.2.assisstments.rds, use the readRDS() function.
asm = readRDS("info5200.2.assisstments.rds")
```

## 1. Problem Identification

In the real world, we usually start by identifying the problem and then collect data. Here we have a dataset to work with. So what problems might we solve? Here are some ideas:

- predict dropout, (how long) will students stay engaged to intervene before they disengage
- predict correctness on first attempt to start adapting content for at-risk students
- predict time spent, predict number of hints for improving the experience

For the purpose of this homework, we are going to predict dropout. It's a common problem and it is at the student-level, which simplifies methodological considerations.

We can set this up two different ways:
- As a regression problem, the outcome can be the number quizzes completed i.e. how far did you get
- As a classification problem, the outcome can be returning after a given point 
-- e.g. of those students who have come in and finish 100 questions, how many are going to do at least 300 questions?

For both outcomes, you will need to assume that you are observing these students for a while (say until they finished 100 questions) and then you try to predict the future. You can use the data you observed to make predictions but nothing thereafter.

## 2. Data Collection

Which of the variables in the dataset will be used. First, what is the outcome? Second, what are the predictors?

Outcomes
- For the regression problem we are interested in the number (i.e. numeric) of quizzes.
- For the classification problem we are interested in whether (i.e. binary) they go on to complete at least 300, after completing 100 questions.

Predictors
- there are no user attributes in this dataset (socio-demographic or other)
- however, you have access to information about quiz-taking that can be used to engineer features

## 3. Feature Engineering

This is where you create the dataset that you will be using in the prediction model. **You need a student-level dataset.** Check out the previous homework to see how to use the group_by and summarise functions from the tidyverse package to achieve this. 

Usually feature engineering focuses on just the predictors, but let's also create the outcomes in this section.

(a) Create a dataset (call it *asm_outcomes*) that has for each student the number of quizzes completed and and indicator of whether that below 300 (i.e. dropped out before). You are looking for a dataset with 912 rows (# of unique students) and three columns: studentID, num_quiz, quiz300. You can refer to the last HW for help.
```{r}
asm_outcomes = asm %>%
  group_by(studentID) %>%
  summarize(num_quiz = n(),
            quiz300 = num_quiz >= 300)

nrow(asm_outcomes)
head(asm_outcomes)
```

(b) Now let's engineer some features to predict dropout. I will leave this up to your creativity. You can create as many features as you an think of. You can also evaluate them by looking at their correlation with the outcome if you like. Here is just one example to get you started. I'll create a feature that is the total time spent so far working on questions. 

However, there is one critical step not to forget. The features can only be computed using data up to the 100th quiz, given the prediction problem. You will need to throw out the rest. First, keep only the first 100 question records for each student. In this dataset, it takes some (cumbersome) data processing because of how the dates are formatted. Here is one way to do it.

We make a timestamp that can be rank ordered. Then we create a variable i that counts the question order for each student. Now that we know the order in which questions were answered, we can filter out all but the first 100.

```{r}
# We first need to go through this tedious process of 
#  dealing with the dates to make them sortable

# convert to character string
asm$start_day = as.character(asm$start_day)  
# split up e.g. 03-OCT-05
start_day_split = strsplit(asm$start_day, split = "-", fixed = T)
# get the day
asm$start_d = unlist(lapply(start_day_split, first))
# get the year, add 20 in front
asm$start_y = paste0(20, unlist(lapply(start_day_split, last))) 
# get/convert month
asm$start_m = match(unlist(lapply(start_day_split, function(x) x[2])), toupper(month.abb)) 
# convert time to character string
asm$start_time = as.character(asm$start_time) 
# concat it all
asm$start_timestamp = paste0(asm$start_y, asm$start_m, asm$start_d, asm$start_time) 

# Compute the order in which students answered questions, keep first 100
asm_sub = asm %>% 
    group_by(studentID) %>%
    mutate(i = rank(start_timestamp, ties.method = "random")) %>%
    filter(i <= 100)
```

Now that you have a dataset with only the information in it that you can use for prediction, you can start engineering features. Below, you should engineer 10-15 features. Be creative, think about what behaviors could signal that a student will/won't drop out.
```{r}
Q_Difficulty <- asm_sub %>%
  group_by(itemid) %>%
  summarize(Q_diff = mean(correctonfirstattempt))

asm_sub <- left_join(asm_sub, Q_Difficulty, by = 'itemid')

asm_sub$start_day <- as.Date(asm_sub$start_day, format="%d-%b-%y")
asm_sub$finish_day <- as.Date(asm_sub$finish_day, format="%d-%b-%y")

# Now using the asm_sub dataset we can finally compute features like total time
asm_features_overall = asm_sub %>% 
    group_by(studentID) %>%
    summarise(
        total_time = sum(seconds),
        avg_hints = mean(hints),
        avg_attempts = mean(attempts),
        avg_correct = mean(correctonfirstattempt),
        sd_attempts = sd(attempts),
        sd_time = sd(seconds),
        #seconds_per_attempt = mean(seconds/attempts),
        #hints_per_attempt = mean(hints/attempts),
        n_days = as.numeric(difftime(max(finish_day), min(start_day))),
        mean_Q_diff = mean(Q_diff))

asm_features_last10 = asm_sub %>%
  filter(i > 90) %>%
  group_by(studentID) %>%
  summarize(total_time_last10 = sum(seconds),
            avg_hints_last10 = mean(hints),
            avg_attempts_last10 = mean(attempts),
            avg_correct_last10 = mean(correctonfirstattempt)
            #seconds_per_attempt_last10 = mean(seconds/attempts),
            #hints_per_attempt_last10 = mean(hints/attempts))
  )

asm_features <- left_join(asm_features_overall, asm_features_last10, by = "studentID")

# check out your features to make sure you don't have 
# missing values and the distributions look reasonable
# if there are missing values (NAs) then you should handle them before moving on
asm_features[is.na(asm_features)] <- 0

summary(asm_features)
nrow(asm_features)
```

Lastly, you will need to merge the two datasets back together: the one with the outcome data and the one with the features. This dataset should have 912 rows.
```{r}
asm_combined = left_join(asm_features, asm_outcomes, by = "studentID")
nrow(asm_combined)
```

## 4. Feature Selection

This step is usually needed when you have thousands of features, or more features than data points. One option is to remove features that are not predictive, another is to combine many weaker features into one stronger one. A common method for the latter is Principle Component Analysis (PCA).

For now, I am assuming you created about 10-15 features in step 3. If you only have 5 or so, go back and come up with more.

Take the opportunity here to evaluate your various features. Check out the correlation, make plots to see if you are perhaps trying to fit a straight line when the relationship is quadratic or cubic. If so, go back and refine your features.
```{r}
outcome_vars = c("num_quiz", "quiz300")
cor(asm_combined)[,outcome_vars]
plot(asm_combined$total_time, asm_combined$num_quiz)
plot(asm_combined$avg_hints, asm_combined$num_quiz)
plot(asm_combined$avg_attempts, asm_combined$num_quiz)
plot(asm_combined$avg_correct, asm_combined$num_quiz)
plot(asm_combined$avg_attempts, asm_combined$num_quiz)
plot(asm_combined$sd_attempts, asm_combined$num_quiz)
plot(asm_combined$sd_time, asm_combined$num_quiz)
plot(asm_combined$n_days, asm_combined$num_quiz)
plot(asm_combined$mean_Q_diff, asm_combined$num_quiz)
plot(asm_combined$total_time_last10, asm_combined$num_quiz)
plot(asm_combined$avg_hints_last10, asm_combined$num_quiz)
plot(asm_combined$avg_attempts_last10, asm_combined$num_quiz)
plot(asm_combined$avg_correct_last10, asm_combined$num_quiz)
```

## 5. Model Selection / Building

Before we can start building models, we need to split our dataset into a training and a test set. (Note that it we should usually do this before feature engineering so that we are not influenced in our choices by data that we shouldn't be seeing. But then we would have to do the engineering twice. So let's just do it here.)

The dataset is now quite small: 912 students. We do want enough data to train our model, so let's do a 80/20 split: 80% training, 20% test. It is important that the split is **random**. Why? Because we want it to be a representative sample. 

```{r}
# Sample 80% of studentIDs for training and the rest is for testing, 
#    you want a vector of studentIDs
ids_train = sample(asm_combined$studentID, size = 912 * 0.8)
    
# Split the dataset into two; use filter() and %in% to select rows
train = asm_combined %>% filter(studentID %in% ids_train)
test = asm_combined %>% filter(!studentID %in% ids_train)
```

### Need a just-in-time R tutorial?

https://www.datacamp.com/community/tutorials/machine-learning-in-r

### Linear regression

To fit a linear regression model, use the lm() function like this:
- lm(outcome ~ predictor1 + predictor2 + predictor3, data = train)

```{r}
m_linreg = lm(num_quiz ~ . - studentID - quiz300, data = train)

# the output are the coefficients:
m_linreg
```

### Logistic regression

To fit a logistic regression model, use the glm() function like this:
- glm(outcome ~ predictor1 + predictor2 + predictor3, data = train, family = "binomial")

```{r}
m_logreg = glm(quiz300 ~ . - studentID - num_quiz, data = train, family = 'binomial')

# the output are the coefficients:
m_logreg
```

### k Nearest Neighbor

To fit a kNN model, use the knn() function from the {class} package. However, note that the syntax starts to get different here, and you would usually do some tuning, e.g. choosing the right value of *k*. For this case, just choose a number between 1 and 5. The function takes the predictor matrix for training and testing, and a vector of outcomes (binary) for training.
- knn(train = training_predictors, test = testing_predictors, cl = training_outcome, k = k)

Important: Do not forget to remove the studentID! It does not generalize well.

```{r}
# install.packages("class") # you may need to install this first
library(class)
m_knn = knn(train = train[, -c(1, 14, 15)], test = test[, -c(1, 14, 15)], 
            cl = train$quiz300, k = 4)

# the output are the predictions:
m_knn
```

### Classification and Regression Trees

To fit a CART model, use the rpart() function from the {rpart} package. The syntax is pretty similar to the linear/logistic regression models. To build a classification tree you specify method as 'class', for a regression tree you specify it as 'anova'. 
- rpart(binary_outcome ~ predictor1 + predictor2 + predictor3, data = train, method = "class")
- rpart(numeric_outcome ~ predictor1 + predictor2 + predictor3, data = train, method = "anova")

Here's an [R tutorial for CART](https://www.statmethods.net/advstats/cart.html).

```{r}
# install.packages("rpart") # you may need to install this first
library(rpart)
m_class_tree = rpart(quiz300 ~ . - studentID - num_quiz, data = train, method = 'class')
m_reg_tree = rpart(num_quiz ~ . - studentID - quiz300, data = train, method = 'anova')

# the output are the decision trees
m_class_tree
m_reg_tree

# you can even plot it!
plot(m_class_tree, uniform = T)
text(m_class_tree, use.n = F, all = TRUE, cex = .8)

plot(m_reg_tree, uniform = T)
text(m_reg_tree, use.n = F, all = TRUE, cex = .8)

# prune the trees to avoid overfitting by limiting tree complexity
cp_class_tree = m_class_tree$cptable[which.min(m_class_tree$cptable[,"xerror"]),"CP"]
m_class_tree_pruned = prune(m_class_tree, cp = cp_class_tree)

cp_reg_tree = m_reg_tree$cptable[which.min(m_reg_tree$cptable[,"xerror"]),"CP"]
m_reg_tree_pruned = prune(m_reg_tree, cp = cp_reg_tree)
```

### Naive Bayes Classifier

To fit an NB model, use the naiveBayes() function from the {e1071} package. The syntax is pretty similar to the linear/logistic regression models again.
- naiveBayes(binary_outcome ~ predictor1 + predictor2 + predictor3, data = train)

Here's an [R tutorial for naive bayes](https://www.r-bloggers.com/understanding-naive-bayes-classifier-using-r/).

```{r}
# install.packages("e1071") # you may need to install this first
library(e1071)
m_nb = naiveBayes(quiz300 ~ total_time + avg_hints + avg_attempts + avg_correct + sd_attempts +
                    sd_time + n_days + mean_Q_diff + total_time_last10 + avg_hints_last10 +
                    avg_attempts_last10 + avg_correct_last10, data = train)

head(train)

# the output are a-prior and conditional probabilities
m_nb
```

## 6. Evaluation

You just trained a number of models and now you want to know which model performs the best on the test set (holdout data). For simplicity, let us just focus on the classification models here.

Get the predictions for each model using the predict() function where the type is 'response' for the logistic model and 'class' for the other models:
- predict(model, newdata = test, type = ...)
```{r}
# logreg: this returns the probability of dropout, so you can set Prob > 0.5 to mean Dropout
p_logreg = predict(m_logreg, newdata = test) > 0.5
# knn: this already has the prediction
p_knn = m_knn
# class tree
p_class_tree = predict(m_class_tree_pruned, newdata = test)[, c('TRUE')] > 0.5
# naive bayes
p_nb = predict(m_nb, newdata = test)
```

Now you can create a contingency matrix for each model and compute the accuracy, recall, and precision:
- Accuracy: (TruePos + TrueNeg) / total
- Recall: TruePos / (TruePos + FalseNeg)
- Precision: TruePos / (TruePos + FalsePos)

```{r}
# here is the confusion matrix for the logreg model:
cm_logreg = table(true = test$quiz300, predicted = p_logreg)

# Get the other ones and then compute the three metrics for each model
cm_knn = table(true = test$quiz300, predicted = p_knn)
cm_class_tree = table(true = test$quiz300, predicted = p_class_tree)
cm_nb = table(true = test$quiz300, predicted = p_nb)

'Logistic Regression'
cm_logreg
'Accuracy'
(cm_logreg[1, 1] + cm_logreg[2, 2])/(cm_logreg[1, 1] + cm_logreg[1, 2] + cm_logreg[2, 1] +
                                       cm_logreg[2, 2])
'Recall'
cm_logreg[2, 2]/(cm_logreg[2, 2] + cm_logreg[2, 1])
'Precision'
cm_logreg[2, 2]/(cm_logreg[2, 2] + cm_logreg[1, 2])

'k-nearest neighbours (k = 4)'
cm_knn
'Accuracy'
(cm_knn[1, 1] + cm_knn[2, 2])/(cm_knn[1, 1] + cm_knn[1, 2] + cm_knn[2, 1] + cm_knn[2, 2])
'Recall'
cm_knn[2, 2]/(cm_knn[2, 2] + cm_knn[2, 1])
'Precision'
cm_knn[2, 2]/(cm_knn[2, 2] + cm_knn[1, 2])

'Classification Tree'
cm_class_tree
'Accuracy'
(cm_class_tree[1, 1] + cm_class_tree[2, 2])/(cm_class_tree[1, 1] + cm_class_tree[1, 2] +
                                               cm_class_tree[2, 1] + cm_class_tree[2, 2])
'Recall'
cm_class_tree[2, 2]/(cm_class_tree[2, 2] + cm_class_tree[2, 1])
'Precision'
cm_class_tree[2, 2]/(cm_class_tree[2, 2] + cm_class_tree[1, 2])

'Naive Bayes'
cm_nb
'Accuracy'
(cm_nb[1, 1] + cm_nb[2, 2])/(cm_nb[1, 1] + cm_nb[1, 2] + cm_nb[2, 1] + cm_nb[2, 2])
'Recall'
cm_nb[2, 2]/(cm_nb[2, 2] + cm_nb[2, 1])
'Precision'
cm_nb[2, 2]/(cm_nb[2, 2] + cm_nb[1, 2])
```

### Briefly summarize your findings

Which model has the highest/lowest accuracy, recall, precision?

My classification tree preformed best in terms of accuracy, my Naive Bayes classifier performed the best in terms recall, and my logistic regression classifier performed the best in terms of precision. Overall, the classification tree gave the best balance of precision and recall. The logistic regression classifier was too conservative, however, and performed the worst in recall. My knn classifier had the lowest accuracy and precision, however, which may have been related to my choice of k = 4.

Overall, I'm unsure of the generalizability of any of these models given the presence of students who did not complete 100 quizzes in the dataset. I engineered features that examined student behaviour on their 90th-100th quizzes because I thought a change in overall behaviour might be indicative of dropout. However, for students that did not complete 90 quizzes, these features were set to 0. Students that did not complete 100 quizzes obviously did not complete 300 quizzes, so including these students in my training and test datasets artificially enhanced the performance of my models in predicting whether students who completed 100 quizzes would complete 300 quizzes.

## Submit Homework

This is the end of the homework. Please **Knit a PDF report** that shows both the R code and R output and upload it on the EdX platform. Alternatively, you can Knit it as a "doc", open it in Word, and save that as a PDF.

**Important:** Be sure that all your code is visible. If the line is too long, it gets cut off. If that happens, organize your code on several lines.
